<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>python_medata.api API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>python_medata.api</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from flask import Blueprint, jsonify, request, send_file, safe_join
from sqlalchemy import or_, exists, and_, not_
from datetime import datetime
from models import db, Insights, Information, Answers, Categories
import pandas as pd
import acm_scraper as scraper
from nltk.corpus import wordnet as wn
import pathlib
import re
import multiprocessing

api = Blueprint(&#39;api&#39;, __name__)

def url_checker(url):
    &#34;&#34;&#34;Modifies the url from a pdf or epdf view to a regular url

    Args:
        String: url of a pdf or edpf view or regular url

    Returns:
        url: regularised url as a paper id
    &#34;&#34;&#34;
    if &#34;epdf/&#34; in url:
        return url.replace(&#34;epdf/&#34;,&#34;&#34;)
    elif &#34;pdf/&#34; in url:
        return url.replace(&#34;pdf/&#34;,&#34;&#34;)
    elif &#34;fullHtml/&#34; in url:
        return url.replace(&#34;fullHtml/&#34;,&#34;&#34;)
    else:
        return url


@api.route(&#39;/ping&#39;, methods=[&#39;GET&#39;])
def ping_pong():
    &#34;&#34;&#34;Check if Server is running

    Returns:
        json: just return a string &#34;pong&#34; in json format
    &#34;&#34;&#34;
    return jsonify(&#39;pong!&#39;)

@api.route(&#39;/get_all&#39;, methods=[&#39;GET&#39;])
def get_all():
    &#34;&#34;&#34;Testing Method to return whole database

    Returns:
        json: complete database sorted by insights
    &#34;&#34;&#34;
    response_object = {&#39;status&#39;: &#39;success&#39;}
    for x in range(1,Insights.query.count()):
        response_object[f&#39;insight {x}&#39;] = Insights.query.get(x).to_dict()
    
    return jsonify(response_object)


@api.route(&#39;/get_specific&#39;, methods=[&#39;POST&#39;])
def get_specific():
    &#34;&#34;&#34;Get all &#39;information&#39; for a specific url (=paper_id)

    Returns:
        json: if no &#39;informatin&#39; is listed for this paper, an Array with the leaf &#39;categories&#39; is returned, otherwise a json object with all relevant &#39;information&#39;
        and the leaf &#39;categories&#39; are returned
    &#34;&#34;&#34;
    #fetch data from request
    url = request.get_json().get(&#39;url&#39;)
    url = url_checker(url)

    #a max of &#39;number_information&#39; is returned
    number_information = 9

    #&#39;information&#39; linked to &#39;insights&#39; which have been downvoted for relevant_categories is not added
    max_downvote_category = 2
    response_information = []

    #scrape leaf &#39;categories&#39;
    relevant_categories = scraper.get_leaf_categories(url)
    paper_id = url

    #query &#39;insights&#39;
    matching_insight = Insights.query.join(Insights.categories).filter(or_(Categories.name==x for x in relevant_categories)).filter(Categories.downvote_category &lt;= max_downvote_category).all()
    #if &#39;information&#39; for paper_id does not exist, create &#39;information&#39; with paper_id
    for x in matching_insight:
        if (Information.query.filter(Information.insight_id==int(x.id)).filter(Information.paper_id==paper_id).count()==0):
            i = Information(insight_id = x.id, 
                            insight_name=x.name, 
                            paper_id=paper_id)
            db.session.add(i)
    db.session.commit()

    #query &#39;information&#39; with and without &#39;answers&#39;
    filtered_information_answers = Information.query.join(Information.answers).filter(or_(Information.insight_id==int(x.id) for x in matching_insight)).filter(Information.paper_id==paper_id).order_by(Answers.answer_score.desc()).all()
    number_information = number_information - len(filtered_information_answers)
    filtered_information_without_answers = Information.query.filter(or_(Information.insight_id==int(x.id) for x in matching_insight)).filter(Information.paper_id==paper_id).filter(Information.answers == None).order_by((Information.insight_upvotes-Information.insight_downvotes).desc()).limit(number_information).all()
    
    #add &#39;information&#39; to response object
    for x in filtered_information_answers:
        response_information.append(x.to_dict())

    for x in filtered_information_without_answers:
        if (x.answers == []):
            response_information.append(x.to_dict())


    response_object = {&#34;metadata&#34;: response_information, &#34;categories&#34;: relevant_categories }
    return jsonify(response_object)



@api.route(&#39;/get_further_information&#39;, methods=[&#39;POST&#39;])
def get_further_information():
    &#34;&#34;&#34;Get scraper to look up more specific information about the url which is posted via POST method
    this includes the title, authors, link to authors profile and the conference. The scraped information is then added
    to the correct &#39;information&#39;


    Returns:
        json: {&#39;status&#39;: &#39;success&#39;}
    &#34;&#34;&#34;
    response_object = {&#39;status&#39;: &#39;success&#39;}
    #url is send from the FE
    url = request.get_json().get(&#39;url&#39;)
    url = url_checker(url)
    paper_id = url_checker(url)
    max_downvote_category = 2
    soup = scraper.get_soup(url)
    relevant_categories = scraper.get_categories(soup)
    #query matching insights
    matching_insight = Insights.query.join(Insights.categories).filter(or_(Categories.name==x for x in relevant_categories)).filter(Categories.downvote_category &lt;= max_downvote_category).all()
    #boolean to indicate whether further information needs to be added and/or scraped
    missing_scraper_information = False
    add_information = False

    #initialize scraper_information 
    authors_profile_link = &#34;&#34;
    authors = &#34;&#34;
    title = &#34;&#34;
    conference = &#34;&#34;
    authors_profile_link = &#34;&#34;
    

    #check if one of the &#39;information&#39; linked to a matching insights has no title, if True scraper_information needs to be added
    for x in matching_insight:
        if (Information.query.filter(Information.insight_id==int(x.id)).filter(Information.paper_id==paper_id).filter(Information.title == &#34;&#34;).count()==1):
            missing_scraper_information = True
            add_information = True
            break

    
    #check if information has been scraped already
    if(missing_scraper_information):
        for x in matching_insight:
            if (Information.query.filter(Information.insight_id==int(x.id)).filter(Information.paper_id==paper_id).filter(Information.title != &#34;&#34;).count()==1):
                current_information = Information.query.filter(Information.insight_id==int(x.id)).filter(Information.paper_id==paper_id).filter(Information.title != &#34;&#34;).first()

                authors_profile_link = current_information.authors_profile_link
                authors = current_information.authors
                title = current_information.title
                conference = current_information.conference
                authors_profile_link = current_information.authors_profile_link
                authors = current_information.authors_profile_link

                missing_scraper_information = False
                break


    #scrape further information
    if (missing_scraper_information): 
        pool = multiprocessing.Pool(multiprocessing.cpu_count())

        facts_soup = scraper.get_facts_soup(soup)
        authors_profile_link = scraper.get_authors(facts_soup)
        authors = []

        start = datetime.now()

        authors = pool.map_async(scraper.name_from_profile,[profile_link for profile_link in authors_profile_link]).get()
        #authors = [scraper.name_from_profile(profile_link) for profile_link in authors_profile_link]

        title = scraper.get_title(facts_soup)
        conference = scraper.get_conference(paper_id)
        authors_profile_link = &#34;--&#34;.join(authors_profile_link)
        authors = &#34;--&#34;.join(authors)


    if(add_information):        
        #add information to &#39;information&#39;
        for x in matching_insight:
            if (Information.query.filter(Information.insight_id==int(x.id)).filter(Information.paper_id==paper_id).filter(Information.title == &#34;&#34;).count()==1):
                current_information = Information.query.filter(Information.insight_id==int(x.id)).filter(Information.paper_id==paper_id).filter(Information.title == &#34;&#34;).first()

                #add title, conference, authors and authors_profile_link to &#39;information&#39;
                current_information.title = title
                current_information.authors = authors
                current_information.authors_profile_link = authors_profile_link
                current_information.conference = conference
                db.session.commit()

    return jsonify(response_object)







@api.route(&#39;/add_insight&#39;, methods =[&#34;POST&#34;])
def add_insight():
    &#34;&#34;&#34;Add an insight to a specific category

    Args:
        json: 
            { 
            &#34;insight&#34; : String with the name of the Insight
            &#34;categories&#34; : List of Strings with category names
            &#34;paper_id&#34; : String with the paper_id which is in our case the completet link to the paper
            }


    Returns:       
        json: {&#39;status&#39;: &#39;success&#39;}
    &#34;&#34;&#34;     
    response_object = {&#39;status&#39;: &#39;success&#39;}
    #fetch data from request
    post_data = request.get_json()
    in_insight_name = post_data.get(&#39;insight&#39;)
    in_categories = post_data.get(&#39;categories&#39;)
    in_paper_id = post_data.get(&#39;paper_id&#39;)
    in_paper_id = url_checker(in_paper_id)


    #create information for paper_id and increment information.insight_upvotes, to make sure the added insight is included
    #array with (insight_upvotes - insight_downvotes) for all &#39;information&#39; listed on paper
    highscore = []
    all_information_paper = Information.query.filter(Information.paper_id == in_paper_id).all()
    for information in all_information_paper:
        highscore.append((information.insight_upvotes - information.insight_downvotes))
    highscore = max(highscore) + 1


    #if insight does not yet exist, add insight, add categories
    if (Insights.query.filter(Insights.name==in_insight_name).count()==0):
        #add insight
        i = Insights(name = str(in_insight_name))
        db.session.add(i)
        db.session.commit()
        for category in in_categories:
            #add categories linked to the above added inisght
            c = Categories(insight_id = i.id, name = str(category))
            db.session.add(c)
        #creats empty information linked to new insight
        inf = Information(insight_id=i.id, insight_name=i.name, paper_id=in_paper_id, insight_upvotes=highscore)
        db.session.add(inf)
        db.session.commit()
    #if insight already exists, add categories if they do no yet exist
    else:
        i = Insights.query.filter(Insights.name==in_insight_name).first()
        for category in in_categories:
            #check if category already exists, if not, add category linked to insight
            if (Categories.query.filter(Categories.insight_id==i.id).filter(Categories.name == str(category)).count()==0):
                c = Categories(insight_id = i.id, name = str(category))
                db.session.add(c)
        #creats empty information linked to existing insight
        inf = Information(insight_id=i.id, insight_name=i.name, paper_id=in_paper_id, insight_upvotes=highscore)
        db.session.add(inf)
        db.session.commit()

    return jsonify(response_object)



@api.route(&#39;/add_answer&#39;, methods = [&#34;POST&#34;])
def add_answer():
    &#34;&#34;&#34;Add a new answer to an existing &#39;information&#39;  

    Args:
        json: 
            { 
            &#34;paper_id&#34; : String with the paper_id which is in our case the completet link to the paper
            &#34;insight&#34; : String with the name of the Insight
            &#34;answer&#34; : String with the Answer
            }


    Returns:       
        json: {&#39;status&#39;: &#39;success&#39;}
    &#34;&#34;&#34;
    response_object = {&#39;status&#39;: &#39;success&#39;}
    #fetch data from request
    post_data = request.get_json()
    in_insight_name = post_data.get(&#39;insight&#39;)
    in_answer = post_data.get(&#39;answer&#39;)
    in_paper_id = post_data.get(&#39;paper_id&#39;)
    in_paper_id = url_checker(in_paper_id)
    answer_already_exists = False

    try:
        in_answer.strip()
    except Exception as e:
        print(f&#34;{e} - given answer is not a String object!&#34;)


    #query &#39;information&#39; 
    inf = Information.query.filter(Information.paper_id==in_paper_id).filter(Information.insight_name==str(in_insight_name)).first()
    #query &#39;answers&#39; linked to &#39;information&#39; 
    ans = Answers.query.filter(Answers.information_id==inf.information_id).all()
  
    #check if the answer already exists
    for a in ans:
        if (a.answer==in_answer):
            answer_already_exists = True

    #if false, add new &#39;answer&#39; linked to &#39;information&#39; with one upvote 
    if (answer_already_exists==False):
        new_answer = Answers(information_id=inf.information_id, answer = in_answer, answer_upvotes = 1, answer_score = 1)
        db.session.add(new_answer)
        db.session.commit()

    return jsonify(response_object)


@api.route(&#39;/rate_answer&#39;, methods = [&#34;POST&#34;])
def rate_answer():
    &#34;&#34;&#34;Rates an already given answer

      Args:
        json: 
            { 
            &#34;insight&#34; : String with the name of the Insight
            &#34;paper_id&#34; : String with the paper_id which is in our case the completet link to the paper
            &#34;upvote&#34; : Boolean if the answer was upvoted(= true) or downvoted (= false)
            &#34;answer&#34; : String with the Answer
            }


    Returns:
        json: {&#39;status&#39;: &#39;success&#39;}
    &#34;&#34;&#34;
    response_object = {&#39;status&#39;: &#39;success&#39;}
    #fetch data from request
    post_data = request.get_json()
    in_insight_name = post_data.get(&#39;insight&#39;)
    in_paper_id = post_data.get(&#39;paper_id&#39;)
    in_paper_id = url_checker(in_paper_id)
    in_upvote = post_data.get(&#39;upvote&#39;)
    in_answer = post_data.get(&#39;answer&#39;)

    #query &#39;information&#39; 
    inf = Information.query.filter(Information.paper_id == in_paper_id).filter(Information.insight_name==str(in_insight_name)).first()
    #query &#39;answers&#39;
    ans = Answers.query.filter(Answers.information_id==inf.information_id).all()

    #upvote correct answer
    if (in_upvote):
        for a in ans:
            if (a.answer==in_answer):
                a.answer_upvotes = a.answer_upvotes + 1
                a.answer_score = a.answer_score + 1

    #downvote correct answer
    else :
        for a in ans:
            if (a.answer==in_answer):
                a.answer_downvotes = a.answer_downvotes + 2
                a.answer_score = a.answer_score - 2

    db.session.commit()
    return jsonify(response_object)


@api.route(&#39;/rate_relevance_insight&#39;, methods = [&#34;POST&#34;])
def rate_relevance_insight():
    &#34;&#34;&#34;Rate the relevance of an already given Insight for a specific paper

      Args:
        json: 
            { 
            &#34;insight&#34; : String with the name of the Insight
            &#34;paper_id&#34; : String with the paper_id which is in our case the completet link to the paper
            &#34;upvote&#34; : Boolean if the insight was upvoted(= true) or downvoted (= false)
            }


    Returns:
        json: {&#39;status&#39;: &#39;success&#39;}
    &#34;&#34;&#34;
    response_object = {&#39;status&#39;: &#39;success&#39;}
    #fetch data from request
    post_data = request.get_json()
    in_insight_name = post_data.get(&#39;insight&#39;)
    in_paper_id = post_data.get(&#39;paper_id&#39;)
    in_paper_id = url_checker(in_paper_id)
    in_upvote = post_data.get(&#39;upvote&#39;)

    #query &#39;information&#39; 
    inf = Information.query.filter(Information.paper_id == in_paper_id).filter(Information.insight_name==str(in_insight_name)).first()

    #upvote 
    if (in_upvote):
        inf.insight_upvotes = inf.insight_upvotes + 1
    #downvote
    else :
        inf.insight_downvotes = inf.insight_downvotes + 1

    db.session.commit()
    return jsonify(response_object)


@api.route(&#39;/download&#39;, methods = [&#34;POST&#34;])
def download():
    &#34;&#34;&#34;download the information of a single or mutitple paper as a csv file

    answer_score_threshold defines the minimum Answer score for the answer to appear in the results. 
    A score of 1 should be the absolute minimum. This score should be set equal to the threshold in the frontend 
    for Insights to be ranked as green.

    FE can either send one url in the json response or a list of urls.
    
    Args:
        json: 
            { 
            &#34;url&#34; : Single url of the page. Does not matter if on epdf, pdf, html or other version of the paper, all work
            &#34;urls_from_binder&#34;: List of urls from the binder
            }

    Returns:
         csv file: includes title, authors names, link to the paper, all Insights and answer with answer_score above the threshold. 
    &#34;&#34;&#34;
    answer_score_threshold = 3
    #fetch data from request
    url = request.get_json().get(&#39;url&#39;)
    if url is not None:
        url = url_checker(url)

    urls_from_binder = request.get_json().get(&#34;urls_from_binder&#34;)

    answer_score_threshold = 4
    #fetch data from request
    url = request.get_json().get(&#39;url&#39;)
    url = url_checker(url)
    urls_from_binder = request.get_json().get(&#34;urls_from_binder&#34;)

    if urls_from_binder is not None:
        urls_from_binder_list = []
        for binder_url in urls_from_binder:
            temp_url = re.search(r&#34;\/doi\/\d*\.\d+\/\d*(\.\d+)*&#34;, binder_url)
            temp_url = &#34;https://dl.acm.org&#34;+temp_url.group()
            urls_from_binder_list.append(temp_url)

        urls = list(set([u.strip() for u in urls_from_binder_list]))
        #removes duplicates
        df = pd.DataFrame()

        for u in urls:
            one_line_df = pd.DataFrame()
            try:
                one_line_df = df_from_url(u)
            except IndexError as ie:
                #If we do not have sufficient information about this paper we return Unknown Title and Author and the Link to the Paper
                no_data = {
                    &#34;Title&#34;: &#34;Unknown&#34;,
                    &#34;Authors&#34;: [&#34;Unknown&#34;],
                    &#34;Link to paper&#34;: u
                }
                one_line_df = pd.DataFrame(data = no_data)
                
            if df.empty:
                df = one_line_df
            else:
                df = pd.concat([df,one_line_df], axis=0, ignore_index=True)
    else:
        try:
            df = df_from_url(url)
        except IndexError as ie:
            no_data = {
                &#34;Title&#34;: &#34;Unknown&#34;,
                &#34;Authors&#34;: [&#34;Unknown&#34;],
                &#34;Link to paper&#34;: url
            }
            df = pd.DataFrame(data = no_data)
             

    path_to_csv = pathlib.Path.cwd() / &#34;exports&#34; / &#34;export_data.csv&#34;
    df.to_csv(pathlib.Path(path_to_csv))
    
    return send_file(safe_join(pathlib.Path(path_to_csv)), as_attachment=True )

    def df_from_url(url):
        url = url
        inf = Information.query.join(Information.answers).filter(Information.paper_id==url).filter(Answers.answer_score &gt; answer_score_threshold).order_by(Answers.answer_score.desc()).all()
        #catch aioor  
        #makes a list of authors splitted by a &#39;,&#39;
        authors = inf[0].authors.replace(&#34;--&#34;, &#34;,&#34;).strip()

        data = {
            &#34;Title&#34;: inf[0].title,
            &#34;Authors&#34;: [authors],
            &#34;Link to paper&#34;: inf[0].paper_id
        }

        for i in inf:
            data[i.insight_name] = i.answers[0].answer

        df = pd.DataFrame(data=data)
        return df
    



@api.route(&#39;/insight_not_relevant_for_category&#39;, methods = [&#34;POST&#34;])
def insight_not_relevant_for_category():
    &#34;&#34;&#34;Downvotes the relevance of an &#39;insight&#39; for a set of &#39;categories&#39;

      Args:
        json: 
            { 
            &#34;insight&#34; : String with the name of the Insight
            &#34;categories&#34; : Array with a set of categories
            }

    Returns:
        json: {&#39;status&#39;: &#39;success&#39;}
    &#34;&#34;&#34;
    response_object = {&#39;status&#39;: &#39;success&#39;}
    #fetch data from request
    post_data = request.get_json()
    in_insight_name = post_data.get(&#39;insight&#39;)
    in_categories = post_data.get(&#39;categories&#39;)

    #query &#39;insight&#39;
    ins = Insights.query.filter(Insights.name==in_insight_name).first()
    #query &#39;categories&#39;
    categories = Categories.query.filter(Categories.insight_id == ins.id).filter(or_(Categories.name==x for x in in_categories)).all()

    #downvote
    for category in categories:
        category.downvote_category = category.downvote_category + 1
    db.session.commit()

    return jsonify(response_object)

    
@api.route(&#39;/typo_error&#39;, methods = [&#39;POST&#39;])
def typ_error():
    &#34;&#34;&#34;Increments typo_error linked to a specific &#39;insight&#39;

      Args:
        json: 
            { 
            &#34;insight&#34; : String with the name of the Insight
            }


    Returns:
        json: {&#39;status&#39;: &#39;success&#39;}
    &#34;&#34;&#34;
    response_object = {&#39;status&#39;: &#39;success&#39;}
    #fetch data from request
    post_data = request.get_json()
    in_insight_name = post_data.get(&#39;insight&#39;)

    #query &#39;insight&#39;
    i = Insights.query.filter(Insights.name==in_insight_name).first()
    #increment typo_error
    i.typo_error = i.typo_error + 1
    db.session.commit()

    return jsonify(response_object)



@api.route(&#39;/autocomplete&#39;, methods = [&#39;POST&#39;])
def autocomplete():
    &#34;&#34;&#34;Creates an Array of Strings used for autocomplete in the FE based on all &#39;insights&#39; and a set of &#39;categories&#39;

      Args:
        json: 
            { 
            &#34;categories&#34; : Array with a set of categories
            }

    Returns:
        Array: Strings with word suggestions
    &#34;&#34;&#34;
    #fetch data from request
    post_data = request.get_json()
    categories = post_data.get(&#39;categories&#39;)

    response_object = []
    base = []

    #query &#39;insights&#39;
    insights = Insights.query.all()

    #add splitted &#39;insights&#39; to response_object
    for i in insights:
        response_object.append(i.name)
        split = i.name.split()
        for s in split:
            base.append(s)

    #add spltted &#39;categories&#39; to response_object
    for c in categories:
        split = c.split()
        for s in split:
            base.append(s)        


    for word in base:
        try: 
            #each synset represents a diff concept
            for ss in wn.synsets(word):
                for x in ss.lemma_names():
                    #words have the form: &#34;research_laboratory&#34;
                    response_object.append(x.capitalize().replace(&#39;_&#39;, &#39; &#39;))
        except LookupError:
            try:
                import nltk
                nltk.download(&#34;wordnet&#34;)
            except:
                pass

    #remove duplicates           
    response_object = list(set(response_object))

    return jsonify(response_object)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="python_medata.api.add_answer"><code class="name flex">
<span>def <span class="ident">add_answer</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Add a new answer to an existing 'information'
</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>json</code></strong></dt>
<dd>{
"paper_id" : String with the paper_id which is in our case the completet link to the paper
"insight" : String with the name of the Insight
"answer" : String with the Answer
}</dd>
</dl>
<p>Returns:
<br>
json: {'status': 'success'}</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@api.route(&#39;/add_answer&#39;, methods = [&#34;POST&#34;])
def add_answer():
    &#34;&#34;&#34;Add a new answer to an existing &#39;information&#39;  

    Args:
        json: 
            { 
            &#34;paper_id&#34; : String with the paper_id which is in our case the completet link to the paper
            &#34;insight&#34; : String with the name of the Insight
            &#34;answer&#34; : String with the Answer
            }


    Returns:       
        json: {&#39;status&#39;: &#39;success&#39;}
    &#34;&#34;&#34;
    response_object = {&#39;status&#39;: &#39;success&#39;}
    #fetch data from request
    post_data = request.get_json()
    in_insight_name = post_data.get(&#39;insight&#39;)
    in_answer = post_data.get(&#39;answer&#39;)
    in_paper_id = post_data.get(&#39;paper_id&#39;)
    in_paper_id = url_checker(in_paper_id)
    answer_already_exists = False

    try:
        in_answer.strip()
    except Exception as e:
        print(f&#34;{e} - given answer is not a String object!&#34;)


    #query &#39;information&#39; 
    inf = Information.query.filter(Information.paper_id==in_paper_id).filter(Information.insight_name==str(in_insight_name)).first()
    #query &#39;answers&#39; linked to &#39;information&#39; 
    ans = Answers.query.filter(Answers.information_id==inf.information_id).all()
  
    #check if the answer already exists
    for a in ans:
        if (a.answer==in_answer):
            answer_already_exists = True

    #if false, add new &#39;answer&#39; linked to &#39;information&#39; with one upvote 
    if (answer_already_exists==False):
        new_answer = Answers(information_id=inf.information_id, answer = in_answer, answer_upvotes = 1, answer_score = 1)
        db.session.add(new_answer)
        db.session.commit()

    return jsonify(response_object)</code></pre>
</details>
</dd>
<dt id="python_medata.api.add_insight"><code class="name flex">
<span>def <span class="ident">add_insight</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Add an insight to a specific category</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>json</code></strong></dt>
<dd>{
"insight" : String with the name of the Insight
"categories" : List of Strings with category names
"paper_id" : String with the paper_id which is in our case the completet link to the paper
}</dd>
</dl>
<p>Returns:
<br>
json: {'status': 'success'}</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@api.route(&#39;/add_insight&#39;, methods =[&#34;POST&#34;])
def add_insight():
    &#34;&#34;&#34;Add an insight to a specific category

    Args:
        json: 
            { 
            &#34;insight&#34; : String with the name of the Insight
            &#34;categories&#34; : List of Strings with category names
            &#34;paper_id&#34; : String with the paper_id which is in our case the completet link to the paper
            }


    Returns:       
        json: {&#39;status&#39;: &#39;success&#39;}
    &#34;&#34;&#34;     
    response_object = {&#39;status&#39;: &#39;success&#39;}
    #fetch data from request
    post_data = request.get_json()
    in_insight_name = post_data.get(&#39;insight&#39;)
    in_categories = post_data.get(&#39;categories&#39;)
    in_paper_id = post_data.get(&#39;paper_id&#39;)
    in_paper_id = url_checker(in_paper_id)


    #create information for paper_id and increment information.insight_upvotes, to make sure the added insight is included
    #array with (insight_upvotes - insight_downvotes) for all &#39;information&#39; listed on paper
    highscore = []
    all_information_paper = Information.query.filter(Information.paper_id == in_paper_id).all()
    for information in all_information_paper:
        highscore.append((information.insight_upvotes - information.insight_downvotes))
    highscore = max(highscore) + 1


    #if insight does not yet exist, add insight, add categories
    if (Insights.query.filter(Insights.name==in_insight_name).count()==0):
        #add insight
        i = Insights(name = str(in_insight_name))
        db.session.add(i)
        db.session.commit()
        for category in in_categories:
            #add categories linked to the above added inisght
            c = Categories(insight_id = i.id, name = str(category))
            db.session.add(c)
        #creats empty information linked to new insight
        inf = Information(insight_id=i.id, insight_name=i.name, paper_id=in_paper_id, insight_upvotes=highscore)
        db.session.add(inf)
        db.session.commit()
    #if insight already exists, add categories if they do no yet exist
    else:
        i = Insights.query.filter(Insights.name==in_insight_name).first()
        for category in in_categories:
            #check if category already exists, if not, add category linked to insight
            if (Categories.query.filter(Categories.insight_id==i.id).filter(Categories.name == str(category)).count()==0):
                c = Categories(insight_id = i.id, name = str(category))
                db.session.add(c)
        #creats empty information linked to existing insight
        inf = Information(insight_id=i.id, insight_name=i.name, paper_id=in_paper_id, insight_upvotes=highscore)
        db.session.add(inf)
        db.session.commit()

    return jsonify(response_object)</code></pre>
</details>
</dd>
<dt id="python_medata.api.autocomplete"><code class="name flex">
<span>def <span class="ident">autocomplete</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates an Array of Strings used for autocomplete in the FE based on all 'insights' and a set of 'categories'</p>
<p>Args:
json:
{
"categories" : Array with a set of categories
}</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Array</code></dt>
<dd>Strings with word suggestions</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@api.route(&#39;/autocomplete&#39;, methods = [&#39;POST&#39;])
def autocomplete():
    &#34;&#34;&#34;Creates an Array of Strings used for autocomplete in the FE based on all &#39;insights&#39; and a set of &#39;categories&#39;

      Args:
        json: 
            { 
            &#34;categories&#34; : Array with a set of categories
            }

    Returns:
        Array: Strings with word suggestions
    &#34;&#34;&#34;
    #fetch data from request
    post_data = request.get_json()
    categories = post_data.get(&#39;categories&#39;)

    response_object = []
    base = []

    #query &#39;insights&#39;
    insights = Insights.query.all()

    #add splitted &#39;insights&#39; to response_object
    for i in insights:
        response_object.append(i.name)
        split = i.name.split()
        for s in split:
            base.append(s)

    #add spltted &#39;categories&#39; to response_object
    for c in categories:
        split = c.split()
        for s in split:
            base.append(s)        


    for word in base:
        try: 
            #each synset represents a diff concept
            for ss in wn.synsets(word):
                for x in ss.lemma_names():
                    #words have the form: &#34;research_laboratory&#34;
                    response_object.append(x.capitalize().replace(&#39;_&#39;, &#39; &#39;))
        except LookupError:
            try:
                import nltk
                nltk.download(&#34;wordnet&#34;)
            except:
                pass

    #remove duplicates           
    response_object = list(set(response_object))

    return jsonify(response_object)</code></pre>
</details>
</dd>
<dt id="python_medata.api.download"><code class="name flex">
<span>def <span class="ident">download</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>download the information of a single or mutitple paper as a csv file</p>
<p>answer_score_threshold defines the minimum Answer score for the answer to appear in the results.
A score of 1 should be the absolute minimum. This score should be set equal to the threshold in the frontend
for Insights to be ranked as green.</p>
<p>FE can either send one url in the json response or a list of urls.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>json</code></strong></dt>
<dd>{
"url" : Single url of the page. Does not matter if on epdf, pdf, html or other version of the paper, all work
"urls_from_binder": List of urls from the binder
}</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>csv file</code></dt>
<dd>includes title, authors names, link to the paper, all Insights and answer with answer_score above the threshold.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@api.route(&#39;/download&#39;, methods = [&#34;POST&#34;])
def download():
    &#34;&#34;&#34;download the information of a single or mutitple paper as a csv file

    answer_score_threshold defines the minimum Answer score for the answer to appear in the results. 
    A score of 1 should be the absolute minimum. This score should be set equal to the threshold in the frontend 
    for Insights to be ranked as green.

    FE can either send one url in the json response or a list of urls.
    
    Args:
        json: 
            { 
            &#34;url&#34; : Single url of the page. Does not matter if on epdf, pdf, html or other version of the paper, all work
            &#34;urls_from_binder&#34;: List of urls from the binder
            }

    Returns:
         csv file: includes title, authors names, link to the paper, all Insights and answer with answer_score above the threshold. 
    &#34;&#34;&#34;
    answer_score_threshold = 3
    #fetch data from request
    url = request.get_json().get(&#39;url&#39;)
    if url is not None:
        url = url_checker(url)

    urls_from_binder = request.get_json().get(&#34;urls_from_binder&#34;)

    answer_score_threshold = 4
    #fetch data from request
    url = request.get_json().get(&#39;url&#39;)
    url = url_checker(url)
    urls_from_binder = request.get_json().get(&#34;urls_from_binder&#34;)

    if urls_from_binder is not None:
        urls_from_binder_list = []
        for binder_url in urls_from_binder:
            temp_url = re.search(r&#34;\/doi\/\d*\.\d+\/\d*(\.\d+)*&#34;, binder_url)
            temp_url = &#34;https://dl.acm.org&#34;+temp_url.group()
            urls_from_binder_list.append(temp_url)

        urls = list(set([u.strip() for u in urls_from_binder_list]))
        #removes duplicates
        df = pd.DataFrame()

        for u in urls:
            one_line_df = pd.DataFrame()
            try:
                one_line_df = df_from_url(u)
            except IndexError as ie:
                #If we do not have sufficient information about this paper we return Unknown Title and Author and the Link to the Paper
                no_data = {
                    &#34;Title&#34;: &#34;Unknown&#34;,
                    &#34;Authors&#34;: [&#34;Unknown&#34;],
                    &#34;Link to paper&#34;: u
                }
                one_line_df = pd.DataFrame(data = no_data)
                
            if df.empty:
                df = one_line_df
            else:
                df = pd.concat([df,one_line_df], axis=0, ignore_index=True)
    else:
        try:
            df = df_from_url(url)
        except IndexError as ie:
            no_data = {
                &#34;Title&#34;: &#34;Unknown&#34;,
                &#34;Authors&#34;: [&#34;Unknown&#34;],
                &#34;Link to paper&#34;: url
            }
            df = pd.DataFrame(data = no_data)
             

    path_to_csv = pathlib.Path.cwd() / &#34;exports&#34; / &#34;export_data.csv&#34;
    df.to_csv(pathlib.Path(path_to_csv))
    
    return send_file(safe_join(pathlib.Path(path_to_csv)), as_attachment=True )

    def df_from_url(url):
        url = url
        inf = Information.query.join(Information.answers).filter(Information.paper_id==url).filter(Answers.answer_score &gt; answer_score_threshold).order_by(Answers.answer_score.desc()).all()
        #catch aioor  
        #makes a list of authors splitted by a &#39;,&#39;
        authors = inf[0].authors.replace(&#34;--&#34;, &#34;,&#34;).strip()

        data = {
            &#34;Title&#34;: inf[0].title,
            &#34;Authors&#34;: [authors],
            &#34;Link to paper&#34;: inf[0].paper_id
        }

        for i in inf:
            data[i.insight_name] = i.answers[0].answer

        df = pd.DataFrame(data=data)
        return df</code></pre>
</details>
</dd>
<dt id="python_medata.api.get_all"><code class="name flex">
<span>def <span class="ident">get_all</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Testing Method to return whole database</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>json</code></dt>
<dd>complete database sorted by insights</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@api.route(&#39;/get_all&#39;, methods=[&#39;GET&#39;])
def get_all():
    &#34;&#34;&#34;Testing Method to return whole database

    Returns:
        json: complete database sorted by insights
    &#34;&#34;&#34;
    response_object = {&#39;status&#39;: &#39;success&#39;}
    for x in range(1,Insights.query.count()):
        response_object[f&#39;insight {x}&#39;] = Insights.query.get(x).to_dict()
    
    return jsonify(response_object)</code></pre>
</details>
</dd>
<dt id="python_medata.api.get_further_information"><code class="name flex">
<span>def <span class="ident">get_further_information</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Get scraper to look up more specific information about the url which is posted via POST method
this includes the title, authors, link to authors profile and the conference. The scraped information is then added
to the correct 'information'</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>json</code></dt>
<dd>{'status': 'success'}</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@api.route(&#39;/get_further_information&#39;, methods=[&#39;POST&#39;])
def get_further_information():
    &#34;&#34;&#34;Get scraper to look up more specific information about the url which is posted via POST method
    this includes the title, authors, link to authors profile and the conference. The scraped information is then added
    to the correct &#39;information&#39;


    Returns:
        json: {&#39;status&#39;: &#39;success&#39;}
    &#34;&#34;&#34;
    response_object = {&#39;status&#39;: &#39;success&#39;}
    #url is send from the FE
    url = request.get_json().get(&#39;url&#39;)
    url = url_checker(url)
    paper_id = url_checker(url)
    max_downvote_category = 2
    soup = scraper.get_soup(url)
    relevant_categories = scraper.get_categories(soup)
    #query matching insights
    matching_insight = Insights.query.join(Insights.categories).filter(or_(Categories.name==x for x in relevant_categories)).filter(Categories.downvote_category &lt;= max_downvote_category).all()
    #boolean to indicate whether further information needs to be added and/or scraped
    missing_scraper_information = False
    add_information = False

    #initialize scraper_information 
    authors_profile_link = &#34;&#34;
    authors = &#34;&#34;
    title = &#34;&#34;
    conference = &#34;&#34;
    authors_profile_link = &#34;&#34;
    

    #check if one of the &#39;information&#39; linked to a matching insights has no title, if True scraper_information needs to be added
    for x in matching_insight:
        if (Information.query.filter(Information.insight_id==int(x.id)).filter(Information.paper_id==paper_id).filter(Information.title == &#34;&#34;).count()==1):
            missing_scraper_information = True
            add_information = True
            break

    
    #check if information has been scraped already
    if(missing_scraper_information):
        for x in matching_insight:
            if (Information.query.filter(Information.insight_id==int(x.id)).filter(Information.paper_id==paper_id).filter(Information.title != &#34;&#34;).count()==1):
                current_information = Information.query.filter(Information.insight_id==int(x.id)).filter(Information.paper_id==paper_id).filter(Information.title != &#34;&#34;).first()

                authors_profile_link = current_information.authors_profile_link
                authors = current_information.authors
                title = current_information.title
                conference = current_information.conference
                authors_profile_link = current_information.authors_profile_link
                authors = current_information.authors_profile_link

                missing_scraper_information = False
                break


    #scrape further information
    if (missing_scraper_information): 
        pool = multiprocessing.Pool(multiprocessing.cpu_count())

        facts_soup = scraper.get_facts_soup(soup)
        authors_profile_link = scraper.get_authors(facts_soup)
        authors = []

        start = datetime.now()

        authors = pool.map_async(scraper.name_from_profile,[profile_link for profile_link in authors_profile_link]).get()
        #authors = [scraper.name_from_profile(profile_link) for profile_link in authors_profile_link]

        title = scraper.get_title(facts_soup)
        conference = scraper.get_conference(paper_id)
        authors_profile_link = &#34;--&#34;.join(authors_profile_link)
        authors = &#34;--&#34;.join(authors)


    if(add_information):        
        #add information to &#39;information&#39;
        for x in matching_insight:
            if (Information.query.filter(Information.insight_id==int(x.id)).filter(Information.paper_id==paper_id).filter(Information.title == &#34;&#34;).count()==1):
                current_information = Information.query.filter(Information.insight_id==int(x.id)).filter(Information.paper_id==paper_id).filter(Information.title == &#34;&#34;).first()

                #add title, conference, authors and authors_profile_link to &#39;information&#39;
                current_information.title = title
                current_information.authors = authors
                current_information.authors_profile_link = authors_profile_link
                current_information.conference = conference
                db.session.commit()

    return jsonify(response_object)</code></pre>
</details>
</dd>
<dt id="python_medata.api.get_specific"><code class="name flex">
<span>def <span class="ident">get_specific</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Get all 'information' for a specific url (=paper_id)</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>json</code></dt>
<dd>if no 'informatin' is listed for this paper, an Array with the leaf 'categories' is returned, otherwise a json object with all relevant 'information'</dd>
</dl>
<p>and the leaf 'categories' are returned</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@api.route(&#39;/get_specific&#39;, methods=[&#39;POST&#39;])
def get_specific():
    &#34;&#34;&#34;Get all &#39;information&#39; for a specific url (=paper_id)

    Returns:
        json: if no &#39;informatin&#39; is listed for this paper, an Array with the leaf &#39;categories&#39; is returned, otherwise a json object with all relevant &#39;information&#39;
        and the leaf &#39;categories&#39; are returned
    &#34;&#34;&#34;
    #fetch data from request
    url = request.get_json().get(&#39;url&#39;)
    url = url_checker(url)

    #a max of &#39;number_information&#39; is returned
    number_information = 9

    #&#39;information&#39; linked to &#39;insights&#39; which have been downvoted for relevant_categories is not added
    max_downvote_category = 2
    response_information = []

    #scrape leaf &#39;categories&#39;
    relevant_categories = scraper.get_leaf_categories(url)
    paper_id = url

    #query &#39;insights&#39;
    matching_insight = Insights.query.join(Insights.categories).filter(or_(Categories.name==x for x in relevant_categories)).filter(Categories.downvote_category &lt;= max_downvote_category).all()
    #if &#39;information&#39; for paper_id does not exist, create &#39;information&#39; with paper_id
    for x in matching_insight:
        if (Information.query.filter(Information.insight_id==int(x.id)).filter(Information.paper_id==paper_id).count()==0):
            i = Information(insight_id = x.id, 
                            insight_name=x.name, 
                            paper_id=paper_id)
            db.session.add(i)
    db.session.commit()

    #query &#39;information&#39; with and without &#39;answers&#39;
    filtered_information_answers = Information.query.join(Information.answers).filter(or_(Information.insight_id==int(x.id) for x in matching_insight)).filter(Information.paper_id==paper_id).order_by(Answers.answer_score.desc()).all()
    number_information = number_information - len(filtered_information_answers)
    filtered_information_without_answers = Information.query.filter(or_(Information.insight_id==int(x.id) for x in matching_insight)).filter(Information.paper_id==paper_id).filter(Information.answers == None).order_by((Information.insight_upvotes-Information.insight_downvotes).desc()).limit(number_information).all()
    
    #add &#39;information&#39; to response object
    for x in filtered_information_answers:
        response_information.append(x.to_dict())

    for x in filtered_information_without_answers:
        if (x.answers == []):
            response_information.append(x.to_dict())


    response_object = {&#34;metadata&#34;: response_information, &#34;categories&#34;: relevant_categories }
    return jsonify(response_object)</code></pre>
</details>
</dd>
<dt id="python_medata.api.insight_not_relevant_for_category"><code class="name flex">
<span>def <span class="ident">insight_not_relevant_for_category</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Downvotes the relevance of an 'insight' for a set of 'categories'</p>
<p>Args:
json:
{
"insight" : String with the name of the Insight
"categories" : Array with a set of categories
}</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>json</code></dt>
<dd>{'status': 'success'}</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@api.route(&#39;/insight_not_relevant_for_category&#39;, methods = [&#34;POST&#34;])
def insight_not_relevant_for_category():
    &#34;&#34;&#34;Downvotes the relevance of an &#39;insight&#39; for a set of &#39;categories&#39;

      Args:
        json: 
            { 
            &#34;insight&#34; : String with the name of the Insight
            &#34;categories&#34; : Array with a set of categories
            }

    Returns:
        json: {&#39;status&#39;: &#39;success&#39;}
    &#34;&#34;&#34;
    response_object = {&#39;status&#39;: &#39;success&#39;}
    #fetch data from request
    post_data = request.get_json()
    in_insight_name = post_data.get(&#39;insight&#39;)
    in_categories = post_data.get(&#39;categories&#39;)

    #query &#39;insight&#39;
    ins = Insights.query.filter(Insights.name==in_insight_name).first()
    #query &#39;categories&#39;
    categories = Categories.query.filter(Categories.insight_id == ins.id).filter(or_(Categories.name==x for x in in_categories)).all()

    #downvote
    for category in categories:
        category.downvote_category = category.downvote_category + 1
    db.session.commit()

    return jsonify(response_object)</code></pre>
</details>
</dd>
<dt id="python_medata.api.ping_pong"><code class="name flex">
<span>def <span class="ident">ping_pong</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Check if Server is running</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>json</code></dt>
<dd>just return a string "pong" in json format</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@api.route(&#39;/ping&#39;, methods=[&#39;GET&#39;])
def ping_pong():
    &#34;&#34;&#34;Check if Server is running

    Returns:
        json: just return a string &#34;pong&#34; in json format
    &#34;&#34;&#34;
    return jsonify(&#39;pong!&#39;)</code></pre>
</details>
</dd>
<dt id="python_medata.api.rate_answer"><code class="name flex">
<span>def <span class="ident">rate_answer</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Rates an already given answer</p>
<p>Args:
json:
{
"insight" : String with the name of the Insight
"paper_id" : String with the paper_id which is in our case the completet link to the paper
"upvote" : Boolean if the answer was upvoted(= true) or downvoted (= false)
"answer" : String with the Answer
}</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>json</code></dt>
<dd>{'status': 'success'}</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@api.route(&#39;/rate_answer&#39;, methods = [&#34;POST&#34;])
def rate_answer():
    &#34;&#34;&#34;Rates an already given answer

      Args:
        json: 
            { 
            &#34;insight&#34; : String with the name of the Insight
            &#34;paper_id&#34; : String with the paper_id which is in our case the completet link to the paper
            &#34;upvote&#34; : Boolean if the answer was upvoted(= true) or downvoted (= false)
            &#34;answer&#34; : String with the Answer
            }


    Returns:
        json: {&#39;status&#39;: &#39;success&#39;}
    &#34;&#34;&#34;
    response_object = {&#39;status&#39;: &#39;success&#39;}
    #fetch data from request
    post_data = request.get_json()
    in_insight_name = post_data.get(&#39;insight&#39;)
    in_paper_id = post_data.get(&#39;paper_id&#39;)
    in_paper_id = url_checker(in_paper_id)
    in_upvote = post_data.get(&#39;upvote&#39;)
    in_answer = post_data.get(&#39;answer&#39;)

    #query &#39;information&#39; 
    inf = Information.query.filter(Information.paper_id == in_paper_id).filter(Information.insight_name==str(in_insight_name)).first()
    #query &#39;answers&#39;
    ans = Answers.query.filter(Answers.information_id==inf.information_id).all()

    #upvote correct answer
    if (in_upvote):
        for a in ans:
            if (a.answer==in_answer):
                a.answer_upvotes = a.answer_upvotes + 1
                a.answer_score = a.answer_score + 1

    #downvote correct answer
    else :
        for a in ans:
            if (a.answer==in_answer):
                a.answer_downvotes = a.answer_downvotes + 2
                a.answer_score = a.answer_score - 2

    db.session.commit()
    return jsonify(response_object)</code></pre>
</details>
</dd>
<dt id="python_medata.api.rate_relevance_insight"><code class="name flex">
<span>def <span class="ident">rate_relevance_insight</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Rate the relevance of an already given Insight for a specific paper</p>
<p>Args:
json:
{
"insight" : String with the name of the Insight
"paper_id" : String with the paper_id which is in our case the completet link to the paper
"upvote" : Boolean if the insight was upvoted(= true) or downvoted (= false)
}</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>json</code></dt>
<dd>{'status': 'success'}</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@api.route(&#39;/rate_relevance_insight&#39;, methods = [&#34;POST&#34;])
def rate_relevance_insight():
    &#34;&#34;&#34;Rate the relevance of an already given Insight for a specific paper

      Args:
        json: 
            { 
            &#34;insight&#34; : String with the name of the Insight
            &#34;paper_id&#34; : String with the paper_id which is in our case the completet link to the paper
            &#34;upvote&#34; : Boolean if the insight was upvoted(= true) or downvoted (= false)
            }


    Returns:
        json: {&#39;status&#39;: &#39;success&#39;}
    &#34;&#34;&#34;
    response_object = {&#39;status&#39;: &#39;success&#39;}
    #fetch data from request
    post_data = request.get_json()
    in_insight_name = post_data.get(&#39;insight&#39;)
    in_paper_id = post_data.get(&#39;paper_id&#39;)
    in_paper_id = url_checker(in_paper_id)
    in_upvote = post_data.get(&#39;upvote&#39;)

    #query &#39;information&#39; 
    inf = Information.query.filter(Information.paper_id == in_paper_id).filter(Information.insight_name==str(in_insight_name)).first()

    #upvote 
    if (in_upvote):
        inf.insight_upvotes = inf.insight_upvotes + 1
    #downvote
    else :
        inf.insight_downvotes = inf.insight_downvotes + 1

    db.session.commit()
    return jsonify(response_object)</code></pre>
</details>
</dd>
<dt id="python_medata.api.typ_error"><code class="name flex">
<span>def <span class="ident">typ_error</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Increments typo_error linked to a specific 'insight'</p>
<p>Args:
json:
{
"insight" : String with the name of the Insight
}</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>json</code></dt>
<dd>{'status': 'success'}</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@api.route(&#39;/typo_error&#39;, methods = [&#39;POST&#39;])
def typ_error():
    &#34;&#34;&#34;Increments typo_error linked to a specific &#39;insight&#39;

      Args:
        json: 
            { 
            &#34;insight&#34; : String with the name of the Insight
            }


    Returns:
        json: {&#39;status&#39;: &#39;success&#39;}
    &#34;&#34;&#34;
    response_object = {&#39;status&#39;: &#39;success&#39;}
    #fetch data from request
    post_data = request.get_json()
    in_insight_name = post_data.get(&#39;insight&#39;)

    #query &#39;insight&#39;
    i = Insights.query.filter(Insights.name==in_insight_name).first()
    #increment typo_error
    i.typo_error = i.typo_error + 1
    db.session.commit()

    return jsonify(response_object)</code></pre>
</details>
</dd>
<dt id="python_medata.api.url_checker"><code class="name flex">
<span>def <span class="ident">url_checker</span></span>(<span>url)</span>
</code></dt>
<dd>
<div class="desc"><p>Modifies the url from a pdf or epdf view to a regular url</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>String</code></strong></dt>
<dd>url of a pdf or edpf view or regular url</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>url</code></dt>
<dd>regularised url as a paper id</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def url_checker(url):
    &#34;&#34;&#34;Modifies the url from a pdf or epdf view to a regular url

    Args:
        String: url of a pdf or edpf view or regular url

    Returns:
        url: regularised url as a paper id
    &#34;&#34;&#34;
    if &#34;epdf/&#34; in url:
        return url.replace(&#34;epdf/&#34;,&#34;&#34;)
    elif &#34;pdf/&#34; in url:
        return url.replace(&#34;pdf/&#34;,&#34;&#34;)
    elif &#34;fullHtml/&#34; in url:
        return url.replace(&#34;fullHtml/&#34;,&#34;&#34;)
    else:
        return url</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="python_medata" href="index.html">python_medata</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="python_medata.api.add_answer" href="#python_medata.api.add_answer">add_answer</a></code></li>
<li><code><a title="python_medata.api.add_insight" href="#python_medata.api.add_insight">add_insight</a></code></li>
<li><code><a title="python_medata.api.autocomplete" href="#python_medata.api.autocomplete">autocomplete</a></code></li>
<li><code><a title="python_medata.api.download" href="#python_medata.api.download">download</a></code></li>
<li><code><a title="python_medata.api.get_all" href="#python_medata.api.get_all">get_all</a></code></li>
<li><code><a title="python_medata.api.get_further_information" href="#python_medata.api.get_further_information">get_further_information</a></code></li>
<li><code><a title="python_medata.api.get_specific" href="#python_medata.api.get_specific">get_specific</a></code></li>
<li><code><a title="python_medata.api.insight_not_relevant_for_category" href="#python_medata.api.insight_not_relevant_for_category">insight_not_relevant_for_category</a></code></li>
<li><code><a title="python_medata.api.ping_pong" href="#python_medata.api.ping_pong">ping_pong</a></code></li>
<li><code><a title="python_medata.api.rate_answer" href="#python_medata.api.rate_answer">rate_answer</a></code></li>
<li><code><a title="python_medata.api.rate_relevance_insight" href="#python_medata.api.rate_relevance_insight">rate_relevance_insight</a></code></li>
<li><code><a title="python_medata.api.typ_error" href="#python_medata.api.typ_error">typ_error</a></code></li>
<li><code><a title="python_medata.api.url_checker" href="#python_medata.api.url_checker">url_checker</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>